{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from transformers import BertTokenizer\n",
    "from torchvision.transforms import Compose, Lambda, Resize, Normalize, ColorJitter\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 8\n",
    "sampling_rate = 8\n",
    "frames_per_second = 30\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(8),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling frames per clip divided over the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_root = '../train'\n",
    "index = 0\n",
    "data = pd.read_csv('../train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KabnUV5luJ8.mp4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[index]['video_id']+\".mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(video_root, data.iloc[index]['video_id']+\".mp4\")\n",
    "label = data.iloc[index]['label']\n",
    "text = data.iloc[index]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_path: ../train/KabnUV5luJ8.mp4\n"
     ]
    }
   ],
   "source": [
    "print(f'video_path: {video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: please name of the software deep fake try putting great thunberg on hitler true it wud be hard to tell dude you are too good cuz she looks like melania trump\n"
     ]
    }
   ],
   "source": [
    "print(f'text: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video using PyTorchVideo\n",
    "video = EncodedVideo.from_path(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pytorchvideo.data.encoded_video_pyav.EncodedVideoPyAV object at 0x786f383f5580>\n"
     ]
    }
   ],
   "source": [
    "print(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 561152/11025\n",
      "step: 70144/11025\n"
     ]
    }
   ],
   "source": [
    "# Get video duration and calculate the step size for frame sampling\n",
    "duration = video.duration\n",
    "step = duration / num_frames\n",
    "print(f'duration: {duration}')\n",
    "print(f'step: {step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m end_sec \u001b[38;5;241m=\u001b[39m start_sec \u001b[38;5;241m+\u001b[39m step\n\u001b[1;32m      6\u001b[0m clip \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mget_clip(start_sec\u001b[38;5;241m=\u001b[39mstart_sec, end_sec\u001b[38;5;241m=\u001b[39mend_sec)\n\u001b[0;32m----> 7\u001b[0m video_data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sample frames at regular intervals\n",
    "video_data = []\n",
    "for i in range(num_frames):\n",
    "    start_sec = i * step\n",
    "    end_sec = start_sec + step\n",
    "    clip = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "    video_data.append(transform(clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(video_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(video_data[0]['video'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the sampled frames\n",
    "# Extract video tensors from each dictionary\n",
    "video_tensors = [item['video'] for item in video_data]\n",
    "\n",
    "# Stack video tensors along the frames dimension\n",
    "stacked_video = torch.stack(video_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_video = stacked_video.squeeze(2).permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 256, 256])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/davendra/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 256, 256])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = stacked_video.unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(input)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = torch.nn.Sequential(*list(model.blocks.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 32, 8, 8])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2 = feature_extractor(input)\n",
    "pred2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 8, 8, 2048])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred3 = pred2.permute(0, 2, 3, 4, 1)\n",
    "pred3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling over the entire video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_root = '../train'\n",
    "index = 0\n",
    "data = pd.read_csv('../train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(video_root, data.iloc[index]['video_id']+\".mp4\")\n",
    "label = data.iloc[index]['label']\n",
    "text = data.iloc[index]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_path: ../train/KabnUV5luJ8.mp4\n"
     ]
    }
   ],
   "source": [
    "print(f'video_path: {video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: please name of the software deep fake try putting great thunberg on hitler true it wud be hard to tell dude you are too good cuz she looks like melania trump\n"
     ]
    }
   ],
   "source": [
    "print(f'text: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pytorchvideo.data.encoded_video_pyav.EncodedVideoPyAV object at 0x78e3398e71d0>\n"
     ]
    }
   ],
   "source": [
    "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n",
    "\n",
    "# Initialize an EncodedVideo helper class and load the video\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "print(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 720, 1280])\n",
      "torch.Size([3, 8, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "print(video_data['video'].shape)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = transform(video_data)\n",
    "\n",
    "print(video_data['video'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFakeDataset(Dataset):\n",
    "    def __init__(self, video_path_file, text_csv_file, text_transforms=None, video_transforms=None, num_frames=8, sampling_rate=8, frames_per_second=30):\n",
    "        # self.root = video_root\n",
    "        self.video_annotation = pd.read_csv(video_path_file)\n",
    "        self.text_df = pd.read_csv(text_csv_file)\n",
    "        self.text_transforms = text_transforms\n",
    "        self.video_transforms = video_transforms\n",
    "        self.num_frames = num_frames\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.frames_per_second = frames_per_second\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_annotation)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        video_path = self.video_annotation.iloc[index]['video_path']\n",
    "        label = self.video_annotation.iloc[index]['label']\n",
    "        text = self.text_df.iloc[index]['text']\n",
    "\n",
    "        try:\n",
    "            # Load video using PyTorchVideo\n",
    "            video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "            # Get video duration and calculate the step size for frame sampling\n",
    "            duration = video.duration\n",
    "            step = duration / self.num_frames\n",
    "\n",
    "            # Sample frames at regular intervals\n",
    "            video_data = []\n",
    "            for i in range(self.num_frames):\n",
    "                start_sec = i * step\n",
    "                end_sec = start_sec + step\n",
    "                clip = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "                print(f'clip shape: {clip['video'].shape}')\n",
    "                video_data.append(self.video_transforms(clip['video']))\n",
    "\n",
    "            # Stack the sampled frames\n",
    "            video_data = torch.stack(video_data)\n",
    "            # print(video_data)\n",
    "            print(f'Video_data shape: {video_data.shape}')\n",
    "            \n",
    "            # Stack the sampled frames\n",
    "            # Extract video tensors from each dictionary\n",
    "            # video_tensors = [item['video'] for item in video_data]\n",
    "\n",
    "            # Stack video tensors along the frames dimension\n",
    "            # video_frames = torch.stack(video_tensors).squeeze(2).permute(1, 0, 2, 3)\n",
    "\n",
    "            # clip_duration = min((num_frames * sampling_rate) / frames_per_second, video.duration)\n",
    "            # start_sec = 0\n",
    "            # end_sec = start_sec + clip_duration\n",
    "\n",
    "            # video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "            # video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "            # Apply a transform to normalize the video input\n",
    "            # if self.video_transforms:\n",
    "            #     video_frames = self.video_transforms(video_data['video'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error Processing video {video_path}: {e}')\n",
    "            # print(f'Clip Duration: {clip_duration}')\n",
    "            # print(f'Video Duration: {video.duration}')\n",
    "\n",
    "        # Apply text transforms\n",
    "        # if self.text_transforms:\n",
    "        #     text_data = self.text_transforms(text)\n",
    "        # else:\n",
    "        \n",
    "        text_data = text\n",
    "\n",
    "        return {\n",
    "            'video': video_data,\n",
    "            'text': text_data,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of text and video transforms\n",
    "text_transforms = Compose([\n",
    "    BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    # Add more text transformations as needed\n",
    "])\n",
    "\n",
    "# video_transforms = Compose([\n",
    "#     ApplyTransformToKey(\n",
    "#         key=\"video\",\n",
    "#         transform=Compose([\n",
    "#             UniformTemporalSubsample(num_video_samples=1),\n",
    "#             RandomResizedCrop(size=224, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "#             RandomHorizontalFlip(p=0.5),\n",
    "#             ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "#             # Add more video transformations as needed\n",
    "#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         ]),\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 30\n",
    "sampling_rate = 8\n",
    "frames_per_second = 30\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "# video_transforms =  ApplyTransformToKey(\n",
    "#     key=\"video\",\n",
    "#     transform=Compose(\n",
    "#         [\n",
    "#             UniformTemporalSubsample(num_frames),\n",
    "#             Lambda(lambda x: x/255.0),\n",
    "#             NormalizeVideo(mean, std),\n",
    "#             ShortSideScale(\n",
    "#                 size=side_size\n",
    "#             ),\n",
    "#             CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "#         ]\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "video_transforms = Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(30),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Create an instance of the dataset\n",
    "video_path_file = '../annotations/video_train_path.csv'\n",
    "text_csv_file = '../annotations/text_train.csv'\n",
    "\n",
    "dataset = DeepFakeDataset(\n",
    "                            video_path_file=video_path_file,\n",
    "                            text_csv_file=text_csv_file,\n",
    "                            text_transforms=text_transforms,\n",
    "                            video_transforms=video_transforms,\n",
    "                            num_frames=num_frames,\n",
    "                            sampling_rate=sampling_rate,\n",
    "                            frames_per_second=frames_per_second\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader\n",
    "batch_size = 4\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip shape: torch.Size([3, 137, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 137, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 137, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 137, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 137, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 137, 720, 1280])\n",
      "clip shape: torch.Size([3, 136, 720, 1280])\n",
      "clip shape: torch.Size([3, 135, 720, 1280])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 44, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "clip shape: torch.Size([3, 43, 1280, 720])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 59, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "clip shape: torch.Size([3, 58, 1280, 720])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "clip shape: torch.Size([3, 72, 352, 640])\n",
      "clip shape: torch.Size([3, 71, 352, 640])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 180, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 180, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 180, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 180, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 180, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "clip shape: torch.Size([3, 179, 720, 1280])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "clip shape: torch.Size([3, 32, 1280, 720])\n",
      "clip shape: torch.Size([3, 31, 1280, 720])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 28, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 28, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 28, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 28, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 28, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 28, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "clip shape: torch.Size([3, 27, 720, 1280])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 29, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 28, 720, 720])\n",
      "clip shape: torch.Size([3, 27, 720, 720])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 495, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 495, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 495, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 496, 720, 1280])\n",
      "clip shape: torch.Size([3, 495, 720, 1280])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 416, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "clip shape: torch.Size([3, 415, 720, 1280])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 41, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 41, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 41, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 41, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 42, 360, 640])\n",
      "clip shape: torch.Size([3, 41, 360, 640])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 161, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "clip shape: torch.Size([3, 160, 720, 1280])\n",
      "Video_data shape: torch.Size([30, 3, 30, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "video_train=list()\n",
    "for data in dataloader:\n",
    "    video_train.append(data['video'])\n",
    "\n",
    "    cnt += 1\n",
    "\n",
    "    if cnt == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30, 3, 30, 256, 256])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_train[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlprj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
