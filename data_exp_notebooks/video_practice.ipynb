{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davendra/anaconda3/envs/mlprj/lib/python3.12/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/davendra/anaconda3/envs/mlprj/lib/python3.12/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from transformers import BertTokenizer\n",
    "from torchvision.transforms import Compose, Lambda, Resize, Normalize, ColorJitter\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 8\n",
    "sampling_rate = 8\n",
    "frames_per_second = 30\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(8),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling frames per clip divided over the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_root = '../train'\n",
    "index = 0\n",
    "data = pd.read_csv('../train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KabnUV5luJ8.mp4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[index]['video_id']+\".mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(video_root, data.iloc[index]['video_id']+\".mp4\")\n",
    "label = data.iloc[index]['label']\n",
    "text = data.iloc[index]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_path: ../train/KabnUV5luJ8.mp4\n"
     ]
    }
   ],
   "source": [
    "print(f'video_path: {video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: please name of the software deep fake try putting great thunberg on hitler true it wud be hard to tell dude you are too good cuz she looks like melania trump\n"
     ]
    }
   ],
   "source": [
    "print(f'text: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video using PyTorchVideo\n",
    "video = EncodedVideo.from_path(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pytorchvideo.data.encoded_video_pyav.EncodedVideoPyAV object at 0x786f383f5580>\n"
     ]
    }
   ],
   "source": [
    "print(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 561152/11025\n",
      "step: 70144/11025\n"
     ]
    }
   ],
   "source": [
    "# Get video duration and calculate the step size for frame sampling\n",
    "duration = video.duration\n",
    "step = duration / num_frames\n",
    "print(f'duration: {duration}')\n",
    "print(f'step: {step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m end_sec \u001b[38;5;241m=\u001b[39m start_sec \u001b[38;5;241m+\u001b[39m step\n\u001b[1;32m      6\u001b[0m clip \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mget_clip(start_sec\u001b[38;5;241m=\u001b[39mstart_sec, end_sec\u001b[38;5;241m=\u001b[39mend_sec)\n\u001b[0;32m----> 7\u001b[0m video_data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sample frames at regular intervals\n",
    "video_data = []\n",
    "for i in range(num_frames):\n",
    "    start_sec = i * step\n",
    "    end_sec = start_sec + step\n",
    "    clip = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "    video_data.append(transform(clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(video_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(video_data[0]['video'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the sampled frames\n",
    "# Extract video tensors from each dictionary\n",
    "video_tensors = [item['video'] for item in video_data]\n",
    "\n",
    "# Stack video tensors along the frames dimension\n",
    "stacked_video = torch.stack(video_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_video = stacked_video.squeeze(2).permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 256, 256])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/davendra/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 256, 256])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = stacked_video.unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(input)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = torch.nn.Sequential(*list(model.blocks.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 32, 8, 8])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2 = feature_extractor(input)\n",
    "pred2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 8, 8, 2048])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred3 = pred2.permute(0, 2, 3, 4, 1)\n",
    "pred3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling over the entire video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_root = '../train'\n",
    "index = 0\n",
    "data = pd.read_csv('../train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(video_root, data.iloc[index]['video_id']+\".mp4\")\n",
    "label = data.iloc[index]['label']\n",
    "text = data.iloc[index]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_path: ../train/KabnUV5luJ8.mp4\n"
     ]
    }
   ],
   "source": [
    "print(f'video_path: {video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: please name of the software deep fake try putting great thunberg on hitler true it wud be hard to tell dude you are too good cuz she looks like melania trump\n"
     ]
    }
   ],
   "source": [
    "print(f'text: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pytorchvideo.data.encoded_video_pyav.EncodedVideoPyAV object at 0x78e3398e71d0>\n"
     ]
    }
   ],
   "source": [
    "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n",
    "\n",
    "# Initialize an EncodedVideo helper class and load the video\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "print(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 720, 1280])\n",
      "torch.Size([3, 8, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "print(video_data['video'].shape)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = transform(video_data)\n",
    "\n",
    "print(video_data['video'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFakeDataset(Dataset):\n",
    "    def __init__(self, csv_file, video_root, text_transforms, video_transforms, num_frames=8, sampling_rate=8, frames_per_second=30):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.video_root = video_root\n",
    "        self.text_transforms = text_transforms\n",
    "        self.video_transforms = video_transforms\n",
    "        self.num_frames = num_frames\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.frames_per_second = frames_per_second\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_path = os.path.join(self.video_root, self.data.iloc[index]['video_id']+\".mp4\")\n",
    "        label = self.data.iloc[index]['label']\n",
    "        text = self.data.iloc[index]['text']\n",
    "\n",
    "        # # Load video using PyTorchVideo\n",
    "        # video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "        # # Get video duration and calculate the step size for frame sampling\n",
    "        # duration = video.duration\n",
    "        # step = duration / self.num_frames\n",
    "\n",
    "        # # Sample frames at regular intervals\n",
    "        # video_data = []\n",
    "        # for i in range(self.num_frames):\n",
    "        #     start_sec = i * step\n",
    "        #     end_sec = start_sec + step\n",
    "        #     clip = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "        #     video_data.append(self.video_transforms(clip))\n",
    "\n",
    "        # # Stack the sampled frames\n",
    "        # # video_data = torch.stack(video_data)\n",
    "\n",
    "        # # Stack the sampled frames\n",
    "        # # Extract video tensors from each dictionary\n",
    "        # video_tensors = [item['video'] for item in video_data]\n",
    "\n",
    "        # # Stack video tensors along the frames dimension\n",
    "        # stacked_video = torch.stack(video_tensors).squeeze(2).permute(1, 0, 2, 3)\n",
    "\n",
    "        clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "        start_sec = 0\n",
    "        end_sec = start_sec + clip_duration\n",
    "\n",
    "        # Initialize an EncodedVideo helper class and load the video\n",
    "        video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "        # video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "        video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "        # Apply a transform to normalize the video input\n",
    "        video_data = transform(video_data)\n",
    "\n",
    "\n",
    "        # Apply text transforms\n",
    "        # text_data = self.text_transforms(text)\n",
    "        text_data = text\n",
    "\n",
    "        return {\n",
    "            'video': video_data['video'],\n",
    "            'text': text_data,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of text and video transforms\n",
    "text_transforms = Compose([\n",
    "    BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    # Add more text transformations as needed\n",
    "])\n",
    "\n",
    "# video_transforms = Compose([\n",
    "#     ApplyTransformToKey(\n",
    "#         key=\"video\",\n",
    "#         transform=Compose([\n",
    "#             UniformTemporalSubsample(num_video_samples=1),\n",
    "#             RandomResizedCrop(size=224, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "#             RandomHorizontalFlip(p=0.5),\n",
    "#             ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "#             # Add more video transformations as needed\n",
    "#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         ]),\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 8\n",
    "sampling_rate = 8\n",
    "frames_per_second = 30\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create an instance of the dataset\n",
    "csv_file = '../train.csv'\n",
    "video_root = '../train'\n",
    "\n",
    "dataset = DeepFakeDataset(csv_file, video_root, text_transforms, transform, num_frames, sampling_rate, frames_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader\n",
    "batch_size = 4\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "video_train=list()\n",
    "for data in dataloader:\n",
    "    video_train.append(data['video'])\n",
    "\n",
    "    cnt += 1\n",
    "\n",
    "    if cnt == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 8, 256, 256])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_train[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlprj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
