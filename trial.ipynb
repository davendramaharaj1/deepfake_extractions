{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davendra/anaconda3/envs/deepfake-detections/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from PIL import Image\n",
    "from MultimodalContextualAttentionNetwork import MultimodalContextualAttention\n",
    "from HierarchicalEncodingNetwork import HierarchicalEncodingNetwork as HMCAN\n",
    "from resnet_50 import ModifiedResNet50 as resnet\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = bert_tokenizer.tokenize(text)\n",
    "\n",
    "    # Add special tokens [CLS] and [SEP]\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "    # Convert tokens to token IDs\n",
    "    input_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    original_input_len = len(input_ids)\n",
    "\n",
    "    # Pad or truncate input IDs to a fixed length\n",
    "    max_length = 512\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    input_ids = input_ids + [0] * padding_length  # Padding token ID for BERT\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = [1]*original_input_len  + [0]*padding_length # 1 for real tokens, 0 for padding tokens\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "    attention_mask = torch.tensor(attention_mask).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'My name is Slim Shady'\n",
    "filename = 'img.jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of id: torch.Size([1, 512])\n",
      "size of attn mask: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "id, attn_mask = preprocess_text(text)\n",
    "print(f'size of id: {id.shape}')\n",
    "print(f'size of attn mask: {attn_mask.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of processed image: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "input_image = Image.open(filename)\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "print(f'size of processed image: {input_batch.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50176, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2 = input_batch.view(input_batch.size(0), input_batch.size(1), -1)  # Changing to [batch, channels, height * width]\n",
    "input2 = input2.permute(0, 2, 1)  # Rearrange to [batch, height * width, channels]\n",
    "input2.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_d_model=768\n",
    "img_d_model=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# Freeze BERT parameters\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = resnet()\n",
    "\n",
    "# Freeze all layers in the feature extractor\n",
    "for param in resnet_model.resnet_feature_extractor.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcan = MultimodalContextualAttention(d_model=text_d_model, nhead=8, dim_feedforward=img_d_model, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmcan = HMCAN(mcan_model=mcan, bert=bert_model, resnet=resnet_model, groups=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = hmcan(id, attn_mask, input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4326, 0.5674]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake-detections",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
