{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib. pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "user_agent = \"Scraper 1.0 by /u/NotoriousKTK\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id = \"hzIhGftozmvyCrBw_n4t9w\",\n",
    "    client_secret = \"1YHFfJX6JEeoikPC62635g5y2KK_cA\",\n",
    "    user_agent = user_agent)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING SUBREDDIT NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "submissions_data = []\n",
    "submission_ids_set = set()  # Set to store submission IDs\n",
    "json_filename = \"reddit_submissions.json\"\n",
    "\n",
    "def extract_comments(comment, submission_id, comments_data):\n",
    "    comments_data.append({\n",
    "        'body': comment.body,\n",
    "        'author': str(comment.author) if comment.author else '[deleted]',\n",
    "        'score': comment.score,\n",
    "        'created_utc': comment.created_utc,\n",
    "        'parent_id': comment.parent_id,\n",
    "        'submission_id': submission_id\n",
    "    })\n",
    "    for reply in comment.replies:\n",
    "        extract_comments(reply, submission_id, comments_data)\n",
    "\n",
    "for submission in reddit.subreddit(\"all\").new(limit = 100):\n",
    "    # Check if the submission ID is already in the set\n",
    "    if submission.id not in submission_ids_set:\n",
    "        submission_data = {\n",
    "            'title': submission.title,\n",
    "            'id': submission.id,\n",
    "            'author': str(submission.author),  # Convert to string\n",
    "            'created_utc': submission.created_utc,\n",
    "            'score': submission.score,\n",
    "            'upvote_ratio': submission.upvote_ratio,\n",
    "            'url': submission.url,\n",
    "            'selftext': submission.selftext,\n",
    "            'num_comments': submission.num_comments,\n",
    "            'link_flair_text': submission.link_flair_text,\n",
    "            'link_flair_css_class': submission.link_flair_css_class,\n",
    "            'locked': submission.locked,\n",
    "            'over_18': submission.over_18,\n",
    "            'spoiler': submission.spoiler,\n",
    "            'subreddit': submission.subreddit.display_name,\n",
    "            'is_video': submission.is_video,\n",
    "            'distinguished': submission.distinguished,\n",
    "            'gilded': submission.gilded,\n",
    "            'comments': []\n",
    "        }\n",
    "\n",
    "        # Add the submission ID to the set\n",
    "        submission_ids_set.add(submission.id)\n",
    "\n",
    "        # Iterate over all comments recursively\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            extract_comments(comment, submission.id, submission_data['comments'])\n",
    "\n",
    "        submissions_data.append(submission_data)\n",
    "\n",
    "\n",
    "\n",
    "with open(json_filename, 'w') as json_file:\n",
    "    json.dump(submissions_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "submissions_data = []\n",
    "submission_ids_set = set()  # Set to store submission IDs\n",
    "keyword = \"Rashmika\"  # This acts similarly to a hashtag.\n",
    "keywords = [\"Rashmika\", \"DeepFakes\", \"Taylor\", \"Trump\", \"Biden\"]\n",
    "\n",
    "json_filename = f\"reddit_{keyword}_submissions.json\"\n",
    "\n",
    "def extract_comments(comment, submission_id, comments_data):\n",
    "    comments_data.append({\n",
    "        'body': comment.body,\n",
    "        'author': str(comment.author) if comment.author else '[deleted]',\n",
    "        'score': comment.score,\n",
    "        'created_utc': comment.created_utc,\n",
    "        'parent_id': comment.parent_id,\n",
    "        'submission_id': submission_id\n",
    "    })\n",
    "    for reply in comment.replies:\n",
    "        extract_comments(reply, submission_id, comments_data)\n",
    "\n",
    "for keyword in keywords:\n",
    "    for submission in reddit.subreddit(\"all\").search(keyword, sort='new', limit=100):\n",
    "        # Check if the submission ID is already in the set\n",
    "        if submission.id not in submission_ids_set:\n",
    "            submission_data = {\n",
    "                'title': submission.title,\n",
    "                'id': submission.id,\n",
    "                'author': str(submission.author),  # Convert to string\n",
    "                'created_utc': submission.created_utc,\n",
    "                'score': submission.score,\n",
    "                'upvote_ratio': submission.upvote_ratio,\n",
    "                'url': submission.url,\n",
    "                'selftext': submission.selftext,\n",
    "                'num_comments': submission.num_comments,\n",
    "                'link_flair_text': submission.link_flair_text,\n",
    "                'link_flair_css_class': submission.link_flair_css_class,\n",
    "                'locked': submission.locked,\n",
    "                'over_18': submission.over_18,\n",
    "                'spoiler': submission.spoiler,\n",
    "                'subreddit': submission.subreddit.display_name,\n",
    "                'is_video': submission.is_video,\n",
    "                'distinguished': submission.distinguished,\n",
    "                'gilded': submission.gilded,\n",
    "                'comments': []\n",
    "            }\n",
    "\n",
    "            # Add the submission ID to the set\n",
    "            submission_ids_set.add(submission.id)\n",
    "\n",
    "            # Iterate over all comments recursively\n",
    "            submission.comments.replace_more(limit=None)\n",
    "            for comment in submission.comments.list():\n",
    "                extract_comments(comment, submission.id, submission_data['comments'])\n",
    "\n",
    "            submissions_data.append(submission_data)\n",
    "\n",
    "with open(json_filename, 'w') as json_file:\n",
    "    json.dump(submissions_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "submissions_data = []\n",
    "submission_ids_set = set()  # Set to store submission IDs along with subreddits\n",
    "base_keywords = [\"Taylor Swift\", \"Trump\", \"Biden\", \"Putin\", \"MaoZeDong\", \"Xi Jinping\"]\n",
    "additional_terms = [\"DeepFake\", \"AI generated\"]  # Additional terms to combine with base keywords\n",
    "media_types = [\"jpg\", \"jpeg\", \"png\", \"gif\", \"gifv\", \"webm\", \"mp4\"]  # Accepted media types\n",
    "\n",
    "# Generate combinations of base keywords with additional terms\n",
    "keyword_combinations = [f\"{keyword} {term}\" for keyword in base_keywords for term in additional_terms]\n",
    "\n",
    "json_filename = f\"reddit_media_with_deepfakes_and_keywords_submissions.json\"\n",
    "\n",
    "def extract_comments(comment, submission_id, comments_data):\n",
    "    comments_data.append({\n",
    "        'body': comment.body,\n",
    "        'author': str(comment.author) if comment.author else '[deleted]',\n",
    "        'score': comment.score,\n",
    "        'created_utc': comment.created_utc,\n",
    "        'parent_id': comment.parent_id,\n",
    "        'submission_id': submission_id\n",
    "    })\n",
    "    for reply in comment.replies:\n",
    "        extract_comments(reply, submission_id, comments_data)\n",
    "\n",
    "for keyword in keyword_combinations:\n",
    "    for submission in reddit.subreddit(\"all\").search(keyword, sort='new', limit=1000):\n",
    "        # Check if the submission ID and subreddit are already in the set\n",
    "        if (submission.id, submission.subreddit.display_name) not in submission_ids_set:\n",
    "            # Check if the submission contains any of the specified media types\n",
    "            if any(media_type in submission.url for media_type in media_types):\n",
    "                submission_data = {\n",
    "                    'title': submission.title,\n",
    "                    'id': submission.id,\n",
    "                    'author': str(submission.author),  # Convert to string\n",
    "                    'created_utc': submission.created_utc,\n",
    "                    'score': submission.score,\n",
    "                    'upvote_ratio': submission.upvote_ratio,\n",
    "                    'url': submission.url,\n",
    "                    'selftext': submission.selftext,\n",
    "                    'num_comments': submission.num_comments,\n",
    "                    'link_flair_text': submission.link_flair_text,\n",
    "                    'link_flair_css_class': submission.link_flair_css_class,\n",
    "                    'locked': submission.locked,\n",
    "                    'over_18': submission.over_18,\n",
    "                    'spoiler': submission.spoiler,\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'is_video': submission.is_video,\n",
    "                    'distinguished': submission.distinguished,\n",
    "                    'gilded': submission.gilded,\n",
    "                    'comments': []\n",
    "                }\n",
    "\n",
    "                # Add the submission ID and subreddit to the set\n",
    "                submission_ids_set.add((submission.id, submission.subreddit.display_name))\n",
    "\n",
    "                # Iterate over all comments recursively\n",
    "                submission.comments.replace_more(limit=None)\n",
    "                for comment in submission.comments.list():\n",
    "                    extract_comments(comment, submission.id, submission_data['comments'])\n",
    "\n",
    "                submissions_data.append(submission_data)\n",
    "\n",
    "with open(json_filename, 'w') as json_file:\n",
    "    json.dump(submissions_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facebook_extract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
